---
laout : single
title: "지역 특징점 검출과 매칭(2)"
excerpt : "키포인트 검출"
categories :
- Computer Vision
tags :
- [Playdata , Python, OpenCV, Computer Vision]
toc: true #Table of contents 옆에 목록이 생성됨
toc_sticky: true #toc가 화면을 따라 내려옴
toc_label: 목차 #toc 상단에 써있는 제목

date: 2022-01-18
last_modified_at: 2022-01-18
---

## 키포인트 검출

```
src = cv2.imread('./data/box_in_scene.png', cv2.IMREAD_GRAYSCALE)
if src is None:
    print("image load file!")
    sys.exit()

orb = cv2.ORB_create()
keypoints = orb.detect(src)
keypoints, descriptor = orb.compute(src, keypoints)

print("len(keypoints) : ",len(keypoints))
print("descriptor.shape : ",descriptor.shape) # keypoint 한개당 256bit의 binary feature vector 생성

# 그리는 함수도 있음
dst = cv2.drawKeypoints(src,keypoints,None,(-1,-1,-1),cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)

cv2.imshow('src',src)
cv2.imshow('dst',dst)
cv2.waitKey()
cv2.destroyAllWindows()

Out :
len(keypoints) :  500
descriptor.shape :  (500, 32)
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/key1.jpg" width="40%">
<img src="/assets/post_photo/opencv/key2.jpg" width="40%">
</div>

## 특징점 매칭

- ORB의 중요 특징
  - Scale Invariance
  - Rotational Invariance
  - Illumination(조명) Invariance
  - Noise Invariance

```
query = cv2.imread("./data/box.png", cv2.IMREAD_GRAYSCALE)
train = cv2.imread("./data/box_in_scene.png", cv2.IMREAD_GRAYSCALE)

if query is None or train is None:
    print("Image load fail")
    sys.exit()

orb = cv2.ORB_create() # default 500개의 keypoint가 생성됨

query_keypoints, query_descriptor = orb.detectAndCompute(query, None)

train_keypoints, train_descriptor = orb.detectAndCompute(train, None)

print("[query] len(keypoints)", len(query_keypoints))
print("[query] descriptor.shape", query_descriptor.shape)

print("[train] len(keypoints)", len(train_keypoints))
print("[train] descriptor.shape", train_descriptor.shape)

matcher = cv2.BFMatcher_create(cv2.NORM_HAMMING) # crossCheck=False
matches = matcher.match(query_descriptor, train_descriptor)

dst = cv2.drawMatches(query, query_keypoints, train, train_keypoints, matches, None) # keypoint와 match가 모두 표시

cv2.imshow('query', query)
cv2.imshow('train', train)
cv2.imshow('dst', dst)
cv2.waitKey()
cv2.destroyAllWindows()

Out :
[query] len(keypoints) 453
[query] descriptor.shape (453, 32)
[train] len(keypoints) 500
[train] descriptor.shape (500, 32)
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/key4.jpg" width="30%">
<img src="/assets/post_photo/opencv/key3.jpg" width="40%">
<img src="/assets/post_photo/opencv/key5.jpg" width="70%">
</div>

## 특징점 매칭 조건 추가

- 두 영상의 특징점 매칭 결과를 정렬
- 상위 50개 매칭 결과 추출
- DrawMatchesFlags : NOT_DRAW_SINGLE_POINTS 설정하여 매칭되지 않은 특징점은 뺀다

```
train_img = cv2.imread('./data/box.png', cv2.IMREAD_GRAYSCALE)
query_img = cv2.imread('./data/box_in_scene.png', cv2.IMREAD_GRAYSCALE)

orb = cv2.ORB_create() # default 500개의 keypoint가 생성됨

train_keypoints, train_descriptor = orb.detectAndCompute(train_img,None)
query_keypoints, query_descriptor = orb.detectAndCompute(query_img,None)

print("[train] len(keypoints) : ",len(train_keypoints))
print("[train] descriptor.shape : ",train_descriptor.shape)

print("[query] len(keypoints) : ",len(query_keypoints))
print("[query] descriptor.shape : ",query_descriptor.shape)

matcher = cv2.BFMatcher_create(cv2.NORM_HAMMING, crossCheck=True)
matches = matcher.match(train_descriptor, query_descriptor)

sorted_matches = sorted(matches, key=lambda x:x.distance)
dst = cv2.drawMatches(train_img, train_keypoints, query_img, query_keypoints, sorted_matches[:50], None,
                      flags = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

cv2.imshow('train_img',train_img)
cv2.imshow('query_img',query_img)
cv2.imshow('dst',dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/key4.jpg" width="30%">
<img src="/assets/post_photo/opencv/key3.jpg" width="40%">
<img src="/assets/post_photo/opencv/key6.jpg" width="70%">
</div>

## 호모그래피와 영상 매칭

- 계산된 호모그래피를 이용하여 박스가 있는 위치를 검출

```
src1 = cv2.imread('./data/box.png', cv2.IMREAD_GRAYSCALE)
src2 = cv2.imread('./data/box_in_scene.png', cv2.IMREAD_GRAYSCALE)

if src1 is None or src2 is None:
    print('Image load failed!')
    sys.exit()

# 특징점 알고리즘 객체 생성 (KAZE, AKAZE, ORB 등)
#feature = cv2.KAZE_create() # 기본값인 L2놈 이용
#feature = cv2.AKAZE_create()
feature = cv2.ORB_create()

# 특징점 검출 및 기술자 계산
kp1, desc1 = feature.detectAndCompute(src1, None)
kp2, desc2 = feature.detectAndCompute(src2, None)

# 특징점 매칭
matcher = cv2.BFMatcher_create()
matches = matcher.match(desc1, desc2)

# 좋은 매칭 결과 선별
matches = sorted(matches, key=lambda x: x.distance)
good_matches = matches[:50]

print('# of kp1:', len(kp1))
print('# of kp2:', len(kp2))
print('# of matches:', len(matches))
print('# of good_matches:', len(good_matches))

# 호모그래피 계산
# DMatch 객체에서 queryIdx와 trainIdx를 받아와서 크기와 타입 변환하기
pts1 = np.array([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2).astype(np.float32)
pts2 = np.array([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2).astype(np.float32)

H, _ = cv2.findHomography(pts1, pts2, cv2.RANSAC) # pts1과 pts2의 행렬 주의 (N,1,2)

# 호모그래피를 이용하여 기준 영상 영역 표시
dst = cv2.drawMatches(src1, kp1, src2, kp2, good_matches, None,
                      flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)

(h, w) = src1.shape[:2]

# 입력 영상의 모서리 4점 좌표
corners1 = np.array([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]
                    ).reshape(-1, 1, 2).astype(np.float32)

# 입력 영상에 호모그래피 H 행렬로 투시 변환
corners2 = cv2.perspectiveTransform(corners1, H)

# corners2는 입력 영상에 좌표가 표현되있으므로 입력영상의 넓이 만큼 쉬프트
corners2 = corners2 + np.float32([w, 0])

# 다각형 그리기
cv2.polylines(dst, [np.int32(corners2)], True, (0, 255, 0), 2, cv2.LINE_AA)

cv2.imshow('dst', dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/key7.jpg" width="70%">
</div>
