---
laout : single
title: "객체 검출"
excerpt : "템플릿 매칭, 케스케이드 분류기와 얼굴 검출, HOG 알고리즘과 보행자 검출"
categories :
- Computer Vision
tags :
- [Playdata , Python, OpenCV, Computer Vision]
toc: true #Table of contents 옆에 목록이 생성됨
toc_sticky: true #toc가 화면을 따라 내려옴
toc_label: 목차 #toc 상단에 써있는 제목

date: 2022-01-14
last_modified_at: 2022-01-14
---


## 템플릿 매칭

- 입력 영상보다 템플릿 이미지가 작아야한다.
- 두 이미지를 매칭할 경우 유사도가 각 좌표에 입력되어 나온다.
- minMaxLoc 을 이용해 좌표를 찾아서 네모박스!

```
import cv2
import sys
import numpy as np
image = cv2.imread('./data/circuit.bmp', cv2.IMREAD_COLOR)
template = cv2.imread('./data/crystal.bmp', cv2.IMREAD_COLOR)

if image is None:
    print("image load failed~")
    sys.exit()

noise = np.zeros(image.shape, image.dtype)
cv2.randn(noise,0,10)
noise = cv2.add(image, noise, dtype=cv2.CV_8UC3)

# 노이즈 추가된 이미지와 템플릿 비교
res = cv2.matchTemplate(noise, template, cv2.TM_CCOEFF_NORMED)

# MIN , MAX 값과 좌표
minv, maxv, minLoc, maxLoc = cv2.minMaxLoc(res)
print("정규화 전", minv, minLoc, maxv, maxLoc)
# MIN,MAX 기준 정규화
res_norm = cv2.normalize(res, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)

# MIN , MAX 값과 좌표
minv, maxv, minLoc, maxLoc = cv2.minMaxLoc(res_norm)
print("정규화 후", minv, minLoc, maxv, maxLoc)

tw, th = template.shape[1],template.shape[0] # or template[:2]

cv2.rectangle(noise,maxLoc, (maxLoc[0]+tw, maxLoc[1]+th),(0,0,255),2)

# 이미지 출력
cv2.imshow('noise',noise)
cv2.imshow('template',template)
cv2.waitKey()
cv2.destroyAllWindows()

Out:
정규화 전 -0.2398252785205841 (665, 345) 0.9984478950500488 (558, 323)
정규화 후 0.0 (665, 345) 255.0 (558, 323)
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/matching1.jpg" width="50%">
<img src="/assets/post_photo/opencv/matching2.jpg" width="30%">
<br /><b> 노이즈가 추가된 입력 영상과 템플릿 영상 </b>
</div>

## 템플릿 매칭 2

```
image = cv2.imread('./data/alphabet.bmp', cv2.IMREAD_COLOR)
template = cv2.imread('./data/S.bmp', cv2.IMREAD_COLOR)

if image is None:
    print("image load failed~")
    sys.exit()

noise = np.zeros(image.shape, image.dtype)
cv2.randn(noise,0,10)
noise = cv2.add(image, noise, dtype=cv2.CV_8UC3)

# 노이즈 추가된 이미지와 템플릿 비교
res = cv2.matchTemplate(noise, template, cv2.TM_CCOEFF_NORMED)
#res = cv2.matchTemplate(noise, template, cv2.TM_CCORR_NORMED)

# MIN , MAX 값과 좌표
minv, maxv, minLoc, maxLoc = cv2.minMaxLoc(res)
print("정규화 전", minv, minLoc, maxv, maxLoc)
# MIN,MAX 기준 정규화
res_norm = cv2.normalize(res, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)

# MIN , MAX 값과 좌표
minv, maxv, minLoc, maxLoc = cv2.minMaxLoc(res_norm)
print("정규화 후", minv, minLoc, maxv, maxLoc)

tw, th = template.shape[1],template.shape[0] # or template[:2]

cv2.rectangle(noise,maxLoc, (maxLoc[0]+tw, maxLoc[1]+th),(0,0,255),2)

cv2.imshow('res_norm',res_norm)
cv2.imshow('template',template)
cv2.imshow('noise',noise)
cv2.waitKey()
cv2.destroyAllWindows()

Out:
정규화 전 -0.27402952313423157 (102, 308) 0.9997914433479309 (280, 146)
정규화 후 0.0 (102, 308) 255.0 (280, 146)
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/matching3.jpg" width="40%">
<br /><b> 원본 영상 </b><br />
<img src="/assets/post_photo/opencv/matching4.jpg" width="40%">
<img src="/assets/post_photo/opencv/matching5.jpg" width="30%">
<br /><b> 매칭 결과 이미지와 템플릿 이미지 </b>
</div>

## 케스케이드 분류기와 얼굴 검출
[haar cascade classifier](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)

[haarcascade Classifier xml 다운로드](http://github.com/opencv/opencv/tree/master/data/haarcascades)

[detectMultiScale 계수 참고 사이트](https://stackoverflow.com/questions/20801015/recommended-values-for-opencv-detectmultiscale-parameters)

### 얼굴 검출

```
image = cv2.imread('./data/lena.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

if image is None:
    print("image load failed~")
    sys.exit()

# haarcascade_frontalface_default.xml 위치를 찾아야한다
face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')
faces = face_cascade.detectMultiScale(gray)

for(x,y,w,h) in faces:
    cv2.rectangle(image, (x,y), (x+w,y+h),(0,0,255),2)

cv2.imshow('image',image)
cv2.waitKey()
cv2.destroyAllWindows()
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/cascade1.jpg" width="70%">
</div>

### 얼굴 검출 2

```
image = cv2.imread('./data/kids2.png')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

if image is None:
    print("image load failed~")
    sys.exit()

# haarcascade_frontalface_default.xml 위치를 찾아야한다
face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')
faces = face_cascade.detectMultiScale(gray)

for(x,y,w,h) in faces:
    cv2.rectangle(image, (x,y), (x+w,y+h),(0,0,255),2)

cv2.imshow('image',image)
cv2.waitKey()
cv2.destroyAllWindows()
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/cascade2.jpg" width="70%">
</div>

### 얼굴 , 눈 검출

```
image = cv2.imread('./data/kids2.png')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# haarcascade_frontalface_default.xml 위치를 찾아야한다
face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('./data/haarcascade_eye.xml')

if face_cascade is None or eye_cascade is None:
    print("XML file load failed~")
    sys.exit()

faces = face_cascade.detectMultiScale(gray) # (gray, 1.05) 이런식으로 값을 줘도 된다.

for(x1 ,y1 ,w1 ,h1 ) in faces:
    cv2.rectangle(image, (x1 ,y1), (x1+w1, y1+h1),(0,0,255),2)
    face = gray[y1 : y1+h1, x1:x1+w1]

    # gray 이미지에서 (face , scaleFactor = 1.1,minNeighbors = 3 ) 이런식으로 해도 되고 face이미지를 컬러에서 따와도 된다
    eyes = eye_cascade.detectMultiScale(face,scaleFactor = 1.1,minNeighbors = 5)

    for(x2, y2, w2, h2) in eyes:
        center = x1 + int(x2+w2/2),y1 + int(y2+h2/2)
        cv2.circle(image,center,int(w2/2),(255,0,0),1,cv2.LINE_AA)

cv2.imshow('image',image)
cv2.waitKey()
cv2.destroyAllWindows()
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/cascade3.jpg" width="70%">
</div>

## 얼굴 , 눈 검출 2

```
image = cv2.imread('./data/Multi_faces.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

# haarcascade_frontalface_default.xml 위치를 찾아야한다
face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('./data/haarcascade_eye.xml')

if face_cascade is None or eye_cascade is None:
    print("XML file load failed~")
    sys.exit()

faces = face_cascade.detectMultiScale(gray,scaleFactor = 1.3,minNeighbors = 8) # (gray, 1.05) 이런식으로 값을 줘도 된다.
#faces = face_cascade.detectMultiScale(gray,minSize=(100,100),maxSize=(120,120)) # 이렇게 사이즈도 가능

for (x1 ,y1 ,w1 ,h1 ) in faces:
    cv2.rectangle(image, (x1 ,y1), (x1+w1, y1+h1),(0,0,255),2)
    face = gray[y1 : y1+h1, x1:x1+w1]
    face_color = image[y1 : y1+h1, x1:x1+w1]
    # gray 이미지에서 (face , scaleFactor = 1.1,minNeighbors = 3 ) 이런식으로 해도 되고 face이미지를 컬러에서 따와도 된다
    eyes = eye_cascade.detectMultiScale(face_color,scaleFactor = 1.11,minNeighbors = 3,minSize=(21,21), maxSize=(40,40))

    for(x2, y2, w2, h2) in eyes:
        center = x1 + int(x2+w2/2),y1 + int(y2+h2/2)
        cv2.circle(image,center,int(w2/2),(255,0,0),1,cv2.LINE_AA)
dst = cv2.resize(image, (int(image.shape[1]*0.5),int(image.shape[0]*0.5)))
cv2.imshow('dst',dst)
cv2.waitKey()
cv2.destroyAllWindows()
```


<div style="text-align:center;">
<img src="/assets/post_photo/opencv/cascade4.jpg" width="70%">
</div>

## Camera Device로부터 얻은 frame에 얼굴 , 눈 검출하기

- 얼굴, 눈 검출 함수 생성

```
def detect_face(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')
    eye_cascade = cv2.CascadeClassifier('./data/haarcascade_eye.xml')

    if face_cascade is None or eye_cascade is None:
        print("XML file load failed~")
        sys.exit()

    faces = face_cascade.detectMultiScale(gray,scaleFactor = 1.3,minNeighbors = 3) # (gray, 1.05) 이런식으로 값을 줘도 된다.
    #faces = face_cascade.detectMultiScale(gray,minSize=(100,100),maxSize=(120,120)) # 이렇게 사이즈도 가능

    for (x1 ,y1 ,w1 ,h1 ) in faces:
        cv2.rectangle(image, (x1 ,y1), (x1+w1, y1+h1),(0,0,255),2)
        face = gray[y1 : y1+h1, x1:x1+w1]
        face_color = image[y1 : y1+h1, x1:x1+w1]
        # gray 이미지에서 (face , scaleFactor = 1.1,minNeighbors = 3 ) 이런식으로 해도 되고 face이미지를 컬러에서 따와도 된다
        eyes = eye_cascade.detectMultiScale(face,scaleFactor = 1.11,minNeighbors = 3)

        for(x2, y2, w2, h2) in eyes:
            center = x1 + int(x2+w2/2),y1 + int(y2+h2/2)
            cv2.circle(image,center,int(w2/2),(255,0,0),1,cv2.LINE_AA)
    cv2.imshow('image',image)
```

- 카메라 실행

```
cap = cv2.VideoCapture(0) # 카메라 디바이스, 동영상 파일명, 스트리밍 주소

# 제대로 연결이 안되면 종료
if not cap.isOpened():
    print("Camera open failed!!")
    sys.exit()

w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

print("frame width, height:", w, h)
print("fps : ",fps)

fourcc = cv2.VideoWriter_fourcc(*"DIVX")

outputVideo = cv2.VideoWriter('./out/output.avi', fourcc, fps, (w,h))

while True:
    ret, frame = cap.read()
    if not ret:
        break;

    outputVideo.write(frame)
    detect_face(frame)

    # 27은 esc 키 정보?
    # waitKey( ? ) 안에는 시간 정보 ms
    if cv2.waitKey(10) == 27:
        break

if cap.isOpened():
      cap.release()

outputVideo.release()
cv2.destroyAllWindows()
```

## Camera Device로부터 얻은 frame에 얼굴에 Blurring 처리

- blur 처리 함수 생성

```
def blur_face(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')
    eye_cascade = cv2.CascadeClassifier('./data/haarcascade_eye.xml')

    if face_cascade is None or eye_cascade is None:
        print("XML file load failed~")
        sys.exit()

    faces = face_cascade.detectMultiScale(gray,scaleFactor = 1.3,minNeighbors = 3) # (gray, 1.05) 이런식으로 값을 줘도 된다.
    #faces = face_cascade.detectMultiScale(gray,minSize=(100,100),maxSize=(120,120)) # 이렇게 사이즈도 가능

    for x, y, w, h in faces:
        # cv2.rectangle(image, rect ,(0,0,255),2)
        image[ y:y+h , x:x+w ] = cv2.blur(image[ y:y+h , x:x+w ], (100,100))
    cv2.imshow('image',image)
```

- 카메라 실행

```
cap = cv2.VideoCapture(0) # 카메라 디바이스, 동영상 파일명, 스트리밍 주소

# 제대로 연결이 안되면 종료
if not cap.isOpened():
    print("Camera open failed!!")
    sys.exit()

w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

print("frame width, height:", w, h)
print("fps : ",fps)

fourcc = cv2.VideoWriter_fourcc(*"DIVX")

outputVideo = cv2.VideoWriter('./out/output.avi', fourcc, fps, (w,h))

while True:
    ret, frame = cap.read()
    if not ret:
        break;

    outputVideo.write(frame)
    blur_face(frame)

    # 27은 esc 키 정보?
    # waitKey( ? ) 안에는 시간 정보 ms
    if cv2.waitKey(10) == 27:
        break

if cap.isOpened():
      cap.release()

outputVideo.release()
cv2.destroyAllWindows()
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/cascade5.jpg" width="70%">
</div>

## HOG 알고리즘과 보행자 검출

HOG 알고리즘은 입력 영상으로부터 크기와 방향이 있는 그래디언트를 계산한다.

방향 성분은 0~180도 까지이며 20도 단위로 구분하여 총 9개의 빈으로 구성된 방향 히스토그램을 구한다.

인접한 네개의 셀을 합쳐서 블록(block) 이라고 정의하고 따라서 하나의 블록에는 36개의 방향 히스토그램 정보가 있습니다.

HOG는 기본적으로 64X128 크기의 영상에서 계산을 한다. 여기에서 총 가로 7개 세로 15개의 블록을 얻을 수 있으므로 105개의 블록을 얻게된다.

결과적으로 105X36 = 3780개의 실수값이 64X128 영상을 표현하는 hog 특징 벡터 역할을 한다.

- people1 에서 hog 데이터 수 확인

```
src = cv2.imread('./data/people1.png')

winSize = (src.shape[1],src.shape[0])
blockSize = (16,16)
blockStride = (8,8)
cellSize = (8,8)
nbins = 9

hog = cv2.HOGDescriptor(winSize,blockSize,blockStride,cellSize,nbins)
hog_descriptor = hog.compute(src)

print(hog_descriptor.shape)

cv2.imshow('src',src)
cv2.waitKey()
cv2.destroyAllWindows()

Out :
(3780,)
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/hog1.jpg" width="40%">
</div>

### HOG 알고리즘을 이용해 동영상에서 여러 사람 검출

```
# random color
def random_color():
    b = np.random.randint(0,255)
    g = np.random.randint(0,255)
    r = np.random.randint(0,255)
    return [b,g,r]

cap = cv2.VideoCapture("./data/vtest.avi") # 카메라 디바이스, 동영상 파일명, 스트리밍 주소

# 제대로 연결이 안되면 종료
if not cap.isOpened():
    print("Camera open failed!!")
    sys.exit()

w, h = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

hog = cv2.HOGDescriptor()
hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector()) #hog.getDefaultPeopleDetector() 똑같음

while True:
    ret, frame = cap.read()
    if not ret:
        break;

    detected ,ret = hog.detectMultiScale(frame)

    for rect in detected:
        cv2.rectangle(frame, rect ,random_color(),3)

    cv2.imshow('frame',frame)

    # 27은 esc 키 정보?
    # waitKey( ? ) 안에는 시간 정보 ms
    if cv2.waitKey(10) == 27:
        break

if cap.isOpened():
      cap.release()

outputVideo.release()
cv2.destroyAllWindows()
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/hog2.jpg" width="40%">
<img src="/assets/post_photo/opencv/hog3.jpg" width="40%">
<img src="/assets/post_photo/opencv/hog4.jpg" width="40%">
<img src="/assets/post_photo/opencv/hog5.jpg" width="40%">
</div>

## skima를 이용한 HOG

```
from skimage.feature import hog
import matplotlib.pyplot as plt

src = cv2.imread('./data/people1.png')

winSize = (src.shape[1],src.shape[0])
blockSize = (16,16)
blockStride = (8,8)
cellSize = (8,8)
nbins = 9

hog_feature, hog_image = hog(src, orientations = 9 , pixels_per_cell =(8,8),
                        cells_per_block=(2,2) ,
                        block_norm = 'L2-Hys', transform_sqrt = False,
                        visualize=True, feature_vector = False)

fig = plt.figure()
plt.subplot(121)
plt.imshow(src)

plt.subplot(122)
plt.imshow(hog_image, cmap="gray")
```

<div style="text-align:center;">
<img src="/assets/post_photo/opencv/hog6.jpg" width="70%">
</div>
